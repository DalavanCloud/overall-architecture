# Integration of SILK component into platform

Deliverable 3.2

Delivery date: 15

## Document History

|Ver.          | Name         | Date         | Remark       |
|--------------|--------------|--------------|--------------|
|v0.1          | Luigi Selmi  | 2014-11-28   | First draft |


## Executive Summary

The 4th and last "expectation of behaviour" specified by Tim Berners-Lee in his proposal to create a web of data was
to "include links to other URIs so that they can discover more things". A particular class of these links are those that
connect different representations of the same thing. Being able to connect records coming from different data sources is
a fundamental objective of any data integration effort starting from putting together records of different spreadsheets to integrating data assets owned by one or more companies and finally merging different views of a subject in the World Wide Web where the AAA slogan must apply: "Anyone can say Anything about Any topic".

The objective of the component development was to provide a support for disambiguating and interlinking entities that are added to a LDP container. It assumes RDF data as input and that the structure of the data is known as well.

## Acronyms and Abbreviations



| Acronym |                 Description                  |
|---------|----------------------------------------------|
| API     | Application Programming Interface            |
| BUAS    | Bern University of Applied Sciences          |
| FP3     | Fusepool P3                                  |
| HTTP    | Hypertext Transfer Protocol                  |
| HTTPS   | Secure Hypertext Transfer Protocol           |
| IRI     | Internationalized Resource Identifier        |
| LDP     | Linked Data Platform                         |
| RDF     | Resource Description Framework               |                                |
| REST    | Representational State Transfer              |
| URI     | Uniform Resource Identifier                  |
| URL     | Uniform Resource Locator                     |
| AKSW    | Agile Knowledge Engineering and Semantic Web |                  |




## Normative namespaces

In this document the prefixes used in [CURIEs](http://www.w3.org/TR/curie/) shall refer the following
IRI prefixed:


| Prefix |                                                                                                                                                                                             Namespace                                                                                                                                                                                              |
|--------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| rdf    | [http://www.w3.org/1999/02/22-rdf-syntax-ns\#](http://www.w3.org/1999/02/22-rdf-syntax-ns)
| rdfs   | [http://www.w3.org/2000/01/rdf-schema\#](http://www.w3.org/2000/01/rdf-schema) |
| xsd    | [http://www.w3.org/2001/XMLSchema\#](http://www.w3.org/2001/XMLSchema)     |
| dct    | [http://purl.org/dc/terms/](http://purl.org/dc/terms/) |
| ldp    | [http://www.w3.org/ns/ldp\#](http://www.w3.org/ns/ldp#)|
| fp3    | [http://vocab.fusepool.info/fp3\#](http://vocab.fusepool.info/fp3#)|
| eldp   | [http://vocab.fusepool.info/eldp\#](http://vocab.fusepool.info/eldp#)   |
| trans  | [http://vocab.fusepool.info/transformer\#](http://vocab.fusepool.info/transformer#) |
| fam    | [http://vocab.fusepool.info/fam\#](http://vocab.fusepool.info/fam#) |

## Background
(Why was this task created, what is the goal of the subtask and how does it contribute to the deliverable )

This task has been proposed to offer a mechanism to a user of the platform to discover entities that have been given different names by different authors that refer to the same thing in order to merge their descriptions and augment the knowledge about the entity. This is a crucial task for any user of the platform since what she likely wants is not a collection of disconnected graphs but a well connected knowledge base. The task of disambiguating entities has been given different name by different communities: record linkage, deduplication, entity resolution and so forth. All the methodologies that have been developed to discover equivalent entities are based on the assumption of the identity of indiscernibles [2] that is attributed to the German philosopher Gottfried Wilhelm Leibniz, that states that there cannot be separate objects that have all their properties exactly the same. In order to ascertain whether two entities are the same two types of inferences can be evaluated. On one hand an identity can be inferred from functional and inverse functional properties when there are some statements that relates two entities and the properties are of the type mentioned. As an example, if the following facts are stated

:Bob :hasFather :John  
:Bob :hasFather :Johnny  

and we know that  

:hasFather rdf:type owl:FunctionalProperty  

the identity :John owl:sameAs :Johnny can be inferred by a reasoner. The problem is that it doesn't happen very often that an entity in a data set is related to an entity in a different data set through a functional property. Inverse functional properties like passport number, driver license, fiscal code, are used more frequently. As an example, if the following facts are stated

:John :hasPassport "A123456"  
:Johnny :hasPassport "A123456"

and we know that

:hasPassport rdf:type owl:InverseFunctionalProperty

then a reasoner can infer :John owl:sameAs :Johnny. That said, such properties are not always available and it can happen that the values contain errors so that it is always better to decide about the identity of two entities on more than one single property. In order to disambiguate two entities by their description, i.e. comparing their properties, we must analyze the following aspects and issues

* the vocabularies used for the descriptions  
* the set of properties that are considered to be enough to distinguish two entities globally  
* missing values, spelling errors and lexical differences in the properties literal values
* quality of the matching
* the number of comparisons to perform
* Implications of identity assertions

### Vocabulary mapping
When comparing the descriptions of two entities it can happen that the same vocabularies are used, or different ones. In the first case a comparison can be performed straightforwardly, once a linkage rule is available, while in the second case a mapping between the terms in the two vocabularies must be defined. This task can be performed before the linking task for example stating that two properties in the two vocabularies are equivalent so that when a statement with the term in the first vocabulary is met by a reasoner another new statement with the equivalent term in the second vocabulary will be added to the data set. The mapping can be defined with the specification languages provided by both SILK and LIMES. In the Semantic Web literature the mapping of terms from different vocabularies is often referred to as schema-level or ontology matching while the task of finding equivalent entities is refereed to as instance matching. A comprehensive description of issues and solutions to the problem of aligning two schemas or ontologies is given in [8].  

### Identity criteria
Starting with the case in which all the entities are described using the same vocabulary, so that there is no ambiguity in the meaning of the properties, we have to decide which properties must be compared in order to distinguish two entities globally in the Semantic Web and Linked Data world. In the relational database world this decision is similar to the definition of a primary key for a tuple. The type of the entities is one of such property that is usually available or can be easily inferred. For entities like persons properties like name, place and date of birth are considered to be enough in most cases. For entities like organizations the same is for the name, the registered office or its address. In case some of these properties are missing we will have to consider the uncertainty in the value that is calculated as a result of the comparisons of all the properties used.

### Similarity metrics
Coming to the third point of the comparison issues, many algorithms have been proposed to determine the similarity between two strings, numbers or other data types. A similarity measure tells us, usually in a numeric format, which is the difference between two property values. Good overviews of such algorithms are given in [5] and [6]. In case the vocabulary used for the properties is different a mapping must be defined between the different terms used for the properties. In a comparison between two entities all the results of the similarity functions are used to determine an overall result that will lead to a decision about their identity. One property can be more relevant in the comparison of two entities so that it can be weighted. As an example, inverse functional property are more relevant than names for persons or organizations. The result of the comparison is used to decide automatically if the two entities are a match or a non match or if a manual intervention is needed to disambiguate the two entities. A matching or linkage rule can be defined setting upper and lower threshold values so that a comparison result above the upper value will trigger a match, a result below the lower threshold will trigger a non- match and finally a value between the lower and upper thresholds will be a possible link that will require a human intervention to be solved. The first theoretical framework in the field of record linkage was proposed by Fellegi and Sunter in [7]. It can happen that a match has been determined while the two entities refer to different objects or that a non match has been found for entities that are the same. These errors are referred to as false positives and false negatives respectively. The objective of the framework described in [7] was to define an optimal linkage rule in which after having set acceptable error levels for the false positives and false negatives it must be possible to minimize the number of possible links that require costly human intervention.

What said so far can be put in a more formal way stating that, given a set S of source entities and a set T of target entities, a similarity measure m(s,t) with values in the interval [0,1], upper and lower thresholds u and l, the linking task can be defined as the automatic classification of pairs of entities (s,t) in two disjoint subsets of S x T, matches M for which m(s,t) > u and non matches U for which m(s,t) < l.

The similarity measure m is the aggregate value of the similarity functions used in a comparison. Each similarity function should return a value in the interval [0,1] in order to be aggregated with all the others used in a comparison and return a value for m in the interval [0,1]. When a similarity function returns a value that is not in the range [0,1] it can be normalized. As an example, the Levenshtein distance returns the minimum number of edits needed to transform a string into the other. A normalized version can be defined as a linear function of the similarity that returns a value 0 if the number of edits equals a threshold, let's say two edits, and up to 1 when no edits are needed.

### Quality of the matching
After a set of properties have been selected for the comparison task, we have to chose the similarity function that we want to use to comparing two property values, the aggregation type of the properties comparison results (average, maximum, weighted average), the upper and lower thresholds for matches and non matches. Settings these parameters corresponds in statistics to a hypothesis testing with a null hypothesis that (s,t) is a non match against the alternative that (s,t) is a match being u the level of significance of the rejection of the null hypothesis. A methodology to evaluate the effectiveness of a linkage rule is to select a subset of the source and target data sets, finding manually all the matches and non-matches, running a linking task against the two subsets and then evaluating whether all the matches and non-matches have been found. We are not assured in any case that a linkage rule will return all and only matches and non-matches. When we run the rules we may find out that some of the links have been missed (false negatives) while some false links have been created (false positives). These two values are used to define precision and recall of the linkage rule.

  P = TP / (TP + FP)  
  R = TP / (TP + FN)  


### Blocking
A brute force approach to the task will result in a number of comparisons equal to the Cartesian product of the two data sets with a time complexity of O(|S||T|) where |S| and |T| are the number of records in the two data sets.
An approach to reduce the number of comparisons is to create blocks of candidates that share some feature in the properties that will be used for the comparison. As an example, the starting characters of names could be used to separate entities in different blocks to compare only those that are in the same block. A simplistic solution like in the example would miss many matches and could result in some blocks with few entities and others with too many so that the problem would not be really solved. More than one property can be used for the blocking.

### Implications of identity assertions
The result of the linkage task will be a set of identities. If two entities A and B are believed to be the same and the owl:sameAs relation is stated between them it is necessary to take into account that the equality relation must be reflexive, symmetric and transitive. Another important characteristic of the equality relation is that the substitution axiom must hold: every sentence that is true of A must be true of B. Translated in RDF this principle entails that for each triple in which A is subject or object a new triple can be inferred by a reasoner with B in place of A and vice versa. The issue with the given semantics of the owl:sameAs property is that there might be good reasons (see [4]) to not believe all the claims about the individual A made by an external data provider and at the same time we are not assured our claims are shared by the external data provider. The decision in the first case could depend on the provenance of the data set.  

## Execution/Implementation
(Describe the strategy to implement the task, describe how the risk known upfront were taken into account, describe the actual implementation, describe all trial and error, discussions, failures etc.)
In order to support the interlinking task we looked at tools and libraries available that could be integrated in the architecture. Our constraints were:

* easy integration as a platform component
* use of RDF data sources from files or SPARQL endpoints
* availability of similarity metrics
* easy configuration of linkage rules
* a license similar to Apache 2.0

Two tools were already known to the developers and generally to the Semantic Web and Linked Data communities: Silk and LIMES.
Silk [9] is a project maintained by the Freie Universität Berlin while LIMES [10] is provided by the AKSW group of Universität Leipzig. LIMES is written in Java while Silk uses Scala as programmimg language that can be easily integrated in java projects. Both frameworks provide tools and libraries to support the interlinking task within applications, have been developed and used in many projects are are well documented and constantly improved. They share the same approach about the configurability of the linkage rules based on two different specification languages in XML. Both specification language provide the following features

* access to RDF data sources as files in different serializations or as SPARQL endpoints
* filters of entities as SPARQL query for both source and target data sets
* properties values transformation utilities (lowercase, uppercase and many others)
* similarity metrics for string and numeric data types
* aggregation operations
* thresholds values

They differs in the type of blocking. Silk supports multi-blocking [11] on the properties used for the comparisons. Silk creates a block of entities pairs for each string or numeric property. Silk adds more blocks to that created for a string property from its bigrams in order to avoid false negatives and improve recall. The blocks are then aggregated in a multi-block. Silk developers declare a linear time complexity of O(|S|+|T|) for Silk using the standard blocking method. In [11] Silk developers have evaluated Silk performances by interlinking 204,109 settlements from DBpedia and 530,606 settlements from LinkedGeoDatafrom reporting a further reduction in the number of comparisons by a factor of 28 compared to standard blocking. LIMES uses the triangle inequality of metric spaces for its blocking algorithm. A subset of entities from the target data set T are chosen as references or exemplars. The dimensions of the space is set by the number of predicates whose value will be used for the comparisons. If name and lastname are used then the dimensions are two. The similarity measure used for the comparisons must satisfy the triangle inequality, i.e the entities are mapped into a metric space. The distances between the exemplars and all the entities in T are calculated and the entities in T are indexed by their distance from the closest exemplar in descending order. Given an entity s ∈ S the distance of s from an exemplar point e is  m(s,e) and m(e,t) is the distance of an entity in T from its exemplar. The threshold value for matching an entities pair (s,t) is θ and the distance between them m(s,t). For the triangle inequality we have

    m(s,e) - m(e,t) ≤ m(s,t) ≤ m(s,e) + m(e,t)    (1)  

The comparisons between s and t are done for all t for which the following inequality holds

    m(s,e) - m(e,t) > θ       (2)  

The time complexity of the algorithm is O(|E| + |S||T|), even worse than the Cartesian product of the source and target data sets, but practically it greatly reduces the number of comparisons to be executed.



* SILK selected as it comes with Apache 2.0 license and provides a web application to create linkage rules (workbench)
   and similarity functions without a metric can be used.
  Description of the SILK framework (Silk Single Machine), Silk Link Specification Language and Silk Workbench.


* Performances. Multi-threaded comparisons. Given the number of cores available in the machine the number of threads can be set to use all the cores. The comparisons (match task) to be evaluated for all the entities in a block partition can be split in different threads. A new thread can be started when a partition is ready)  


## Solution
(Describe the solution in detail, start with the benefit the solution provides, then go to the technical details.)

* Implementation of the transformer (based on the REST architecture)
* Description of the tool (SILK)
* Description of the Linkage Rules (discriminative properties, distance measure,thresholds)
* Description of the transformer

## Future work
(Briefly outline how this solution or the way to implement it could be improved by others.)

* Use of other types of topological relationships by developing an extension for Jena or Virtuoso
* Use of OpenStreetMap or LinkedGeoData for interlinking  
* Use of SILK learning capabilities to define linkage rules
* Parallelization of compaisons: GPUs, Map/Reduce. SILK MapReduce caches the entity pairs on the distributed file system provided by Hadoop and then starts the interlinking process.)


## References
[1] [Tim Berners-Lee - Linked Data](http://www.w3.org/DesignIssues/LinkedData.html)  
[2] [Identity of indiscernibles](http://plato.stanford.edu/entries/identity-indiscernible/)  
[3] D. Allemang, J. Hendler - Semantic Web for the Working Ontologist  
[4] H. Alpin, I. Herman, P. Hayes - When owl:sameAs isn't the Same: An Analysis of Identity Links on the Semantic Web  
[5] A. Elmagarmid et al. - Duplicate Record Detection: A Survey  
[6] W. Winkler - Matching and Record Linkage  
[7] I. Fellegi, A. Sunter - A Theory For Record Linkage  
[8] J. Euzenat, P. Shvaiko - Ontology Matching  
[9] [Silk - A Link Discovery framework for the Web of Data](http://wifo5-03.informatik.uni-mannheim.de/bizer/silk/)  
[10] [LIMES - LInk discovery framework for MEtric Spaces](http://aksw.org/Projects/LIMES.html)  
[11] [R. Isele, et al. - Efficient Multidimensional Blocking for Link Discovery without losing   Recall](http://wifo5-03.informatik.uni-mannheim.de/bizer/pub/IseleJentzschBizer-WebDB2011.pdf)  
[12] [A. Ngonga Ngomo, S. Auer - LIMES - A Time-Efficient Approach for Large-Scale Link Discovery on the Web of Data](http://svn.aksw.org/papers/2011/WWW_LIMES/public.pdf)  
